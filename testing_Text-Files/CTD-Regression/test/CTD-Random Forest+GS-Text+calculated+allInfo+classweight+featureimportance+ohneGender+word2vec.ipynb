{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T15:06:09.436033Z",
     "start_time": "2025-01-16T13:00:10.078838Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import shap\n",
    "\n",
    "# Helper function to extract data from text files\n",
    "def load_data(file_pattern, base_path):\n",
    "    data = []\n",
    "    record_ids = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if re.match(file_pattern, file):\n",
    "                with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    data.append(f.read())\n",
    "                match = re.search(r'(Process-rec-\\d{3})', file)\n",
    "                if match:\n",
    "                    record_ids.append(match.group(1))\n",
    "    return data, record_ids\n",
    "\n",
    "# Helper function to load labels\n",
    "def load_labels(label_file):\n",
    "    df = pd.read_csv(label_file)\n",
    "    return df.set_index('Record-ID')[['Class', 'Age', 'Converted-MMSE']]\n",
    "\n",
    "# Feature engineering: Calculate text and timing features\n",
    "def count_interrupters_and_clean_text(text):\n",
    "    interrupter_count = len(re.findall(r'\\(.*?\\)', text))  # Count interruptions in parentheses\n",
    "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)  # Remove parentheses content\n",
    "    return interrupter_count, cleaned_text\n",
    "\n",
    "def find_repetitions_and_timing(text):\n",
    "    interrupter_count, cleaned_text = count_interrupters_and_clean_text(text)\n",
    "    words = re.findall(r'\\b\\w+\\b', cleaned_text)\n",
    "    stutter_words = len(re.findall(r'\\b(er|um|oh|ah|mm)\\b', cleaned_text))\n",
    "    meaningful_words = len(words) - stutter_words\n",
    "    unique_words = len(set(words))\n",
    "    word_count = len(words)\n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'stutter_words': stutter_words,\n",
    "        'meaningful_words': meaningful_words,\n",
    "        'unique_words': unique_words,\n",
    "        'interrupter_count': interrupter_count,\n",
    "    }\n",
    "\n",
    "def extract_features(texts):\n",
    "    return pd.DataFrame([find_repetitions_and_timing(text) for text in texts])\n",
    "\n",
    "# Load and combine data\n",
    "base_path = \"../process/PROCESS-V1/\"\n",
    "file_pattern = r\"Process-rec-\\d+__CTD\\.txt\"\n",
    "data, record_ids = load_data(file_pattern, base_path)\n",
    "\n",
    "label_file = \"../process/PROCESS-V1/dem-info-filled-mmse-score.csv\"\n",
    "labels = load_labels(label_file)\n",
    "\n",
    "combined_data = []\n",
    "for text, record_id in zip(data, record_ids):\n",
    "    if record_id in labels.index:\n",
    "        row = labels.loc[record_id]\n",
    "        combined_data.append({\n",
    "            'Record-ID': record_id,\n",
    "            'Text': text,\n",
    "            'Class': row['Class'],\n",
    "            'Age': row['Age'],\n",
    "            'Converted-MMSE': row['Converted-MMSE'],\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Entferne fehlerhafte Daten\n",
    "df = df[df['Record-ID'] != 'Process-rec-071'].reset_index(drop=True)\n",
    "\n",
    "# Entferne fehlende Daten\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Zielvariable und Features\n",
    "X_text = df['Text']\n",
    "X_numeric = df[['Age']]\n",
    "y = df['Converted-MMSE']\n",
    "\n",
    "# Split data\n",
    "X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(\n",
    "    X_text, X_numeric, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Word Embeddings with TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "sentences = [text.split() for text in X_train_text]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)\n",
    "\n",
    "def text_to_embedding(text, model):\n",
    "    words = text.split()\n",
    "    embeddings = [model.wv[word] for word in words if word in model.wv]\n",
    "    if embeddings:\n",
    "        return pd.Series(embeddings).mean(axis=0)\n",
    "    else:\n",
    "        return pd.Series([0] * model.vector_size)\n",
    "\n",
    "X_train_text_features = pd.DataFrame([text_to_embedding(text, word2vec_model) for text in X_train_text])\n",
    "X_test_text_features = pd.DataFrame([text_to_embedding(text, word2vec_model) for text in X_test_text])\n",
    "\n",
    "# Extract calculated features\n",
    "X_train_calculated_features = extract_features(X_train_text).reset_index(drop=True)\n",
    "X_test_calculated_features = extract_features(X_test_text).reset_index(drop=True)\n",
    "\n",
    "# Combine features\n",
    "X_train_combined = pd.concat([\n",
    "    X_train_text_features,\n",
    "    X_train_calculated_features,\n",
    "    X_train_numeric.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test_combined = pd.concat([\n",
    "    X_test_text_features,\n",
    "    X_test_calculated_features,\n",
    "    X_test_numeric.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Ensure column names are strings\n",
    "X_train_combined.columns = X_train_combined.columns.astype(str)\n",
    "X_test_combined.columns = X_test_combined.columns.astype(str)\n",
    "\n",
    "# XGBoost with advanced hyperparameters\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('regressor', XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'regressor__subsample': [0.8, 1.0],\n",
    "    'regressor__colsample_bytree': [0.8, 1.0],\n",
    "    'regressor__gamma': [0, 0.1, 0.2],\n",
    "    'regressor__min_child_weight': [1, 5, 10],\n",
    "    'regressor__reg_alpha': [0, 0.1, 1],\n",
    "    'regressor__reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_combined, y_train)\n",
    "\n",
    "# Best model evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "print(f\"Explained Variance Score: {explained_var}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8748 candidates, totalling 43740 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefa\\OneDrive\\Dokumente\\_Studium\\Master\\Semester 2\\Speech and Language Processing\\Git PSTA\\SLP\\venv\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'regressor__colsample_bytree': 1.0, 'regressor__gamma': 0, 'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__min_child_weight': 10, 'regressor__n_estimators': 200, 'regressor__reg_alpha': 0.1, 'regressor__reg_lambda': 1, 'regressor__subsample': 0.8}\n",
      "Mean Squared Error (MSE): 4.729161210423399\n",
      "Mean Absolute Error (MAE): 1.794454574584961\n",
      "R-squared (R²): -0.12124590865329021\n",
      "Explained Variance Score: -0.12107280819519284\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:06:09.535116Z",
     "start_time": "2025-01-16T15:06:09.527496Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "77dff54d21dd8801",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
